{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEG 2016 Facies Competition\n",
    "\n",
    "\n",
    "In 2016 Matt @ Agile & Brendon @ Enthought setup a Machine Learning Contest with the SEG. \n",
    "\n",
    "The objective was to predict facies logs from a small set of well log data. The image below (source: www.agilescientific.com) shows the wirelines and facies targets for one well\n",
    "\n",
    "![](agile_blog_seg_facies_image.png)\n",
    "\n",
    "Two wells were held out as blind and used to create the final scores.\n",
    "\n",
    "![](leaderboard.png)\n",
    "\n",
    "In this notebook, we've taken the winning submission from `LA_Team` Lukas Mosser & Alfredo de la Fuente.\n",
    "\n",
    "This Was Gradient Boosted Trees selected and tuned by TPOT.\n",
    "\n",
    "here we are going to tune that model with Raytune instead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dependencies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already applied the preprocessing steps from the contest entry notebook and saved these to `h5py` so we can load these here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "\n",
    "# define a loading function\n",
    "def setup(filepath):\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        X_train = f[\"train_x\"][:]\n",
    "        y_train = f[\"train_y\"][:]\n",
    "        group_train = f[\"train_groups\"][:]\n",
    "        train_wells = f[\"train_groups\"].attrs[\"well_names\"]        \n",
    "        \n",
    "        X_test = f[\"test_x\"][:]\n",
    "        y_test = None\n",
    "#         y_test = f[\"test_y\"]\n",
    "        group_test = f[\"test_groups\"][:]\n",
    "        test_wells = f[\"test_groups\"].attrs[\"well_names\"]\n",
    "\n",
    "    return X_train, y_train, group_train, X_test, y_test, group_test, (train_wells, test_wells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned model parameters\n",
    "```\n",
    "XGBClassifier(learning_rate=0.12,\n",
    "              max_depth=3,\n",
    "              min_child_weight=10,\n",
    "              n_estimators=150,\n",
    "              seed=seed,\n",
    "              colsample_bytree=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data is on the expected path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lena/git/tutorial-raytune-hyper/datasets/seg_2016_facies/la_team_5_data.h5py\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "filepath = path.abspath('../datasets/seg_2016_facies/la_team_5_data.h5py')\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning function\n",
    "\n",
    "Again we define an end to end tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from scipy.signal import medfilt\n",
    "from filelock import FileLock\n",
    "\n",
    "def e2e_train_and_test(config, **kwargs):\n",
    "    \n",
    "    # load the data\n",
    "    X, y, groups, X_test, y_test, group_test, well_names = setup(kwargs['filepath'])\n",
    "    \n",
    "    #\n",
    "    # chose your CV strategy. groups == wells\n",
    "    #\n",
    "    splitter = LeavePGroupsOut(1)\n",
    "    \n",
    "    #\n",
    "    # run k fold training and validation\n",
    "    #\n",
    "    f1_scores = [] # keep hold of all individual scores\n",
    "    for train_ind, val_ind in splitter.split(X, y, groups=groups):\n",
    "        pipeline = make_pipeline(RobustScaler(),\n",
    "                                  XGBClassifier())\n",
    "\n",
    "        pipeline.set_params(**config)\n",
    "        pipeline.fit(X[train_ind], y[train_ind])\n",
    "        \n",
    "        y_pred = pipeline.predict(X[val_ind])\n",
    "        \n",
    "        f1_scores.append(f1_score(y_pred, y[val_ind], average='micro'))\n",
    "    \n",
    "        # Clean isolated facies for each well\n",
    "        y_pred = medfilt(y_pred, kernel_size=5)\n",
    "\n",
    "    # use tunes reporter to send metric to tune.run()\n",
    "    tune.report(mean_f1_score=np.array(f1_scores).mean(),\n",
    "                std_f1_score=np.array(f1_scores).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following config is locked to the winning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_config = {\n",
    "    'xgbclassifier__learning_rate': 0.12,\n",
    "    'xgbclassifier__max_depth': 3,\n",
    "    'xbgclassifier__min_child_weight' :10,\n",
    "    'xbgclassifier__n_estimators': 150,\n",
    "    'xgbclassifier__seed':1773,\n",
    "    'xgbclassifier__colsample_bytree':0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update that to use distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_tuning_config = {\n",
    "    'xgbclassifier__learning_rate': tune.loguniform(0.001, 0.5),\n",
    "    'xgbclassifier__max_depth': tune.randint(1, 10),\n",
    "    'xgbclassifier__min_child_weight': tune.loguniform(0.1,100),\n",
    "    'xgbclassifier__n_estimators': tune.randint(5,200),\n",
    "    'xgbclassifier__colsample_bytree': tune.choice([0.4, 0.6, 0.8, 1.0]),\n",
    "    'xgbclassifier__lambda': tune.choice([0,1]),\n",
    "    'xgbclassifier__seed': 42   # set as a constant, so it's always the same\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 15:59:46,932\tINFO services.py:1164 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.123.68',\n 'raylet_ip_address': '192.168.123.68',\n 'redis_address': '192.168.123.68:17328',\n 'object_store_address': '/tmp/ray/session_2020-11-09_15-59-46_197278_28024/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2020-11-09_15-59-46_197278_28024/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2020-11-09_15-59-46_197278_28024',\n 'metrics_export_port': 63124}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(num_cpus=5, num_gpus=0, include_dashboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = path.abspath('../datasets/seg_2016_facies/la_team_5_data.h5py')\n",
    "\n",
    "# wrap our end to end function to inject our filepath\n",
    "def e2e_seg(config):\n",
    "    return e2e_train_and_test(config, filepath=filepath)\n",
    "\n",
    "analysis = tune.run(\n",
    "                e2e_seg,\n",
    "                config=ray_tuning_config,\n",
    "    \n",
    "                num_samples=15, # Specify the number of samples to make from (non grid) distributions\n",
    "    \n",
    "                resources_per_trial=dict(cpu=1, gpu=0),\n",
    "                \n",
    "                local_dir=\"~/ray_results/seg_facies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-4c989f897fd7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpprint\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpprint\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Best config: \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mpprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_best_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmetric\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"mean_f1_score\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(\"Best config: \")\n",
    "pprint(analysis.get_best_config(metric=\"mean_f1_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "top_n_df = df.nlargest(10, \"mean_f1_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_tune_results(top_n_df, (0.3, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from tensorboard import notebook \n",
    "%tensorboard --logdir \"~/ray_results/seg_facies\"\n",
    "notebook.display(height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}